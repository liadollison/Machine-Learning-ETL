# Machine-Learning-ETL
Here you will find the results of group projects in Machine Learning using Dataiku and ETL using Apache Spark. 

## Machine Learning Project File Index
> 1. Machine Learning PDF
>> Contains the visualizations carried out for the EDA section and detailing of the steps carried out

> 2. https://drive.google.com/file/d/1k9ImrJ0QJuLJ_DtHGDm-LyMQJ0Hq3RKJ/view?usp=share_link
>> This is a link to all files associated to the Machine Learning Project. Due to its size, I felt that a google drive provided easier access for the viewer.

## ML Project Description 
>> In this project we worked with a real-world dataset from the Kaggle platform corresponding to the “AMS 2013-2014 Solar Energy Prediction Contest”. Our two main objectives were to perform Exploratory Data Analysis, EDA, clean data, and data pre-processing steps on the dataset and train a Machine Learning model to try to predict solar energy production of 98 stations using the given dataset. The 98 columns (from 2nd to 99th position) give the real values of solar production recorded in 98 different weather stations. These columns are only informed until 2007-12-31 (row 5113); after this date these 98 columns contain NA or missing values. These missing values are the ones that we predicted for the stations ACME, GOOD, and WYNO using the Machine Learning Model.

## ETL Project File Index
> 1.Gas Final Jupyter Notebook
>> All code used to analyze and solve the real use case. Including each step of ingestion using NiFi, data storage using HDFS, and data processing using Apache Spark 
> 2. Modern Data Architectures I
>> Includes a slide deck summarizing our work which was shared with the class and professor

## ETL Project Descriptiom 
>> The main objective of this project was to get deeper look into a real data problem from the technical point of view in order to learn and understand a potential solution. We were required to process data and get insights from our chosen dataset by using some of the technologies we learned durinhg the term (such as those listen in point 1 of the index). We presented the type of analysis we wanted to perform, the raw data formats, the method of data ingestion, our NiFi workflow, the organization of data in HDFS, and of course the notebook of code. 
